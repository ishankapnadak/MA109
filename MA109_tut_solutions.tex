\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsfonts, amsthm, mathtools,mathrsfs}
\usepackage{thmtools}
\usepackage[utf8]{inputenc}
\usepackage[inline]{enumitem}
\usepackage[colorlinks=true]{hyperref}
\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows.meta}
\usepackage{witharrows}
\usepackage[useregional, showdow]{datetime2}
\usepackage{physics}
\DTMlangsetup[en-GB]{abbr}
\usepackage{xcolor}
\usepackage[normalem]{ulem}


\setlength\parindent{0pt}
\usepackage{parskip}

\def\D{\mathrm{d}}

\usepackage[framemethod=tikz]{mdframed}
\mdfdefinestyle{theoremstyle}{%
	% linecolor=gray,linewidth=1pt,%
	% frametitlerule=true,%
	frametitlebackgroundcolor=white,
	% backgroundcolor=  gray!20,	
	bottomline=false, topline=false, rightline=false, leftline=true,
	innerlinewidth=0.7pt, outerlinewidth=0.7pt, middlelinewidth=2pt, middlelinecolor=white, %
	innerleftmargin=6pt,
	% innertopmargin=-1pt,
	skipabove=10pt,
	% fontcolor=blue,
	% innerbottommargin=-0.5pt,
}
\mdtheorem[style=theoremstyle]{defn}[thm]{Definition}
\mdtheorem[style=theoremstyle]{lem}[thm]{Lemma}
\mdtheorem[style=theoremstyle]{thm}{Theorem}

\newcommand*{\doublerule}{\hrule width \hsize height 1pt \kern 0.5mm \hrule width \hsize height 2pt}
\newcommand{\doublerulefill}{\leavevmode\leaders\vbox{\hrule width .1pt\kern1pt\hrule}\hfill\kern0pt}
\def\ddfrac#1#2{\displaystyle\frac{\displaystyle #1}{\displaystyle #2}}


%\newcommand{\Res}{\operatorname{Res}}

\theoremstyle{definition}
% \numberwithin{thm}{section}
% \newtheorem{lem}[thm]{Lemma}
% \newtheorem{defn}[thm]{Definition}
% \newtheorem{prop}[thm]{Proposition}
% \newtheorem{cor}[thm]{Corollary}
% \newtheorem{ex}{Example}


\let\emptyset\varnothing

\usepackage{titlesec}
\titleformat{\section}[block]{\Large\filcenter\bfseries}{\S\thesection.}{0.25cm}{\Large}
\titleformat{\subsection}[block]{\large\bfseries\sffamily}{\S\S\thesubsection.}{0.2cm}{\large}

\usepackage[a4paper]{geometry}
\usepackage{lipsum}
\usepackage{xcolor,cancel}

\usepackage{cleveref}
\crefname{thm}{Theorem}{Theorems}
\crefname{lem}{Lemma}{Lemmas}
\crefname{defn}{Definition}{Definitions}
\crefname{prop}{Proposition}{Propositions}
\crefname{cor}{Corollary}{Corollaries}
\crefname{equation}{}{}

\usepackage{mdframed}
\newenvironment{blockquote}
{\begin{mdframed}[skipabove=0pt, skipbelow=0pt, innertopmargin=4pt, innerbottommargin=4pt, bottomline=false,topline=false,rightline=false, linewidth=2pt]}
{\end{mdframed}}
\newenvironment{soln}{\begin{proof}[Solution]}{\end{proof}}

\title{MA 109: Calculus - I\\\large{Tutorial Solutions}}
\author{Ishan Kapnadak}
\date{Autumn Semester 2020-21\\~\\Updated on: \textcolor{blue}{\DTMToday}}

\begin{document}
\tikzset{lab dis/.store in=\LabDis,
  lab dis=-0.4,
  ->-/.style args={at #1 with label #2}{decoration={
    markings,
    mark=at position #1 with {\arrow{>}; \node at (0,\LabDis) {#2};}},postaction={decorate}},
  -<-/.style args={at #1 with label #2}{decoration={
    markings,
    mark=at position #1 with {\arrow{<}; \node at (0,\LabDis)
    {#2};}},postaction={decorate}},
  -*-/.style args={at #1 with label #2}{decoration={
    markings,
    mark=at position #1 with {{\fill (0,0) circle (1.5pt);} \node at (0,\LabDis)
    {#2};}},postaction={decorate}},
  }
\maketitle
\tableofcontents
\newpage\section{Week 1}
\begin{center}
	25th November, 2020
\end{center}
\textbf{Sheet 1.}
\begin{enumerate}[leftmargin=*]
    \itemsep0.5em
    \item[2 (iv)] $\displaystyle\lim_{n\to \infty}(n)^{1/n}.$
    \begin{soln}
        We will utilise the fact that $n^{1/n} \geq 1$ for all $n \in \mathbb{N}$. (Why is this true?) We define $h_n \coloneqq n^{1/n} - 1$. Then, $h_n \geq 0$ for all $n \in \mathbb{N}$. For $n\geq2$, we have
        \[
            n = (1+h_n)^n \geq 1 + \binom{n}{1} h_n + \binom{n}{2} h_n^2 > \binom{n}{2} h_n^2 = \frac{n(n-1)}{2} h_n^2
        \]
        Cancelling out the $n$'s, we get
        \[
            h_n^2 < \frac{2}{n-1} \implies h_n < \sqrt{\frac{2}{n-1}}
        \]  
        Thus for $n \geq 2$, we have
        \[
            0 \leq h_n < \sqrt{\frac{2}{n-1}}
        \]
        Notice that the limit of the sequence on the right exists and is equal to $0$. Thus, utilising Sandwich Theorem, we get that $\lim\limits_{n \to \infty} h_n = 0$. Recalling how we defined $h_n$, we get $\lim\limits_{n \to \infty} n^{1/n} = 1$.
    \end{soln}
    
    \newpage
    
    \item[3 (ii)] Prove that the sequence $a_n \coloneqq \left\{ (-1)^n \left( \ddfrac{1}{2} - \ddfrac{1}{n} \right) \right\}_{n \geq 1}$ is not convergent.
    
    \begin{soln}
        We will prove this result by contradiction. First, observe that the sequence $b_n \coloneqq \ddfrac{(-1)^n}{n}$ is convergent and its limit is $0$. This is true because its absolute value behaves the same way as $\ddfrac{1}{n}$ (try proving this with the $\epsilon$-$N$ definition to work out the details). We also know that the sequence $\{(-1)^n\}_{n \geq 1}$ is not convergent. (Why?) Now, let us assume that the given sequence $(a_n)$ converges. We have
        \[
            a_n \coloneqq \left\{ (-1)^n \left( \ddfrac{1}{2} - \ddfrac{1}{n} \right) \right\} = \frac{(-1)^n}{2} - \frac{(-1)^n}{n}
        \]
        We also know that the the sum of two convergent sequences is convergent. Since $a_n$ is assumed to be convergent and $b_n$ is convergent, we have that $c_n \coloneqq a_n + b_n = \ddfrac{(-1)^n}{2}$ must also converge. However, the convergence of $c_n$ implies that the sequence $(-1)^n$ also converges. Hence, we arrive at a contradiction and thus, the sequence $(a_n)$ is not convergent.
        
    \end{soln}
    
    \newpage
    
    \item[5 (iii)] Prove that the following sequence is convergent by showing that it is monotone and bounded. Also find its limit. 
    \[
        a_1 = 2, a_{n+1} = 3 + \frac{a_n}{2} \; \forall n \in \mathbb{N}
    \]
    
    \begin{soln}
        We first claim that $a_n < 6$ for all $n \in \mathbb{N}$. To prove this, we will use mathematical induction. The base case, $n=1$ is immediate as $2<6$. Assume that the claim holds for some $n=k$. Now, 
        \[
            a_{k+1} = 3 + \frac{a_k}{2} < 3 + \frac{6}{2} = 6
        \]
        By induction, the claim follows. Hence, $a_n$ is bounded above. 
        \medskip
        
        Next, we claim that $a_{n+1} > a_n$ for all $n \in \mathbb{N}$. We have
        \[
            a_{n+1} - a_n = 3 - \frac{a_n}{2} = \frac{6-a_n}{2}
        \]
        We just showed that $a_n < 6$ for all $n \in \mathbb{N}$. It thus follows that $a_{n+1} > a_n$ for all $n \in \mathbb{N}$. Hence, $(a_n)$ is a monotonically increasing sequence that is bounded above. Thus, it must converge. To find the limit of $(a_n)$, we utilise the fact that $\lim\limits_{n \to \infty} a_{n+1} = \lim\limits_{n \to \infty} a_n$ (Sheet $1$ : Problem $6$). Let $L$ denote the limit of $(a_n)$. Taking the limit of the recursive definition (and using some limit properties), we have that
        \[
            L = 3 + \frac{L}{2} \implies L = 6
        \]
        Thus, the sequence $(a_n)$ converges to 6. (Notice that this was the upper bound we chose for $(a_n)$)
    \end{soln}
    
    \newpage
    
    \item[7] If $\lim\limits_{n \to \infty} a_n = L \neq 0$, show that there exists $n_0 \in \mathbb{N}$ such that
    \[
        \abs{a_n} \geq \frac{\abs{L}}{2}, \quad \forall n \geq n_0
    \]
    
    \begin{soln}
        We will use the $\epsilon-N$ definition to prove this result. Choose $\epsilon = \ddfrac{\abs{L}}{2}$. Since $L \neq 0$, we have $\epsilon > 0$. Now, as $a_n \to L$, there exists $n_0 \in \mathbb{N}$ such that $\abs{a_n - L} < \epsilon$ for all $n \geq n_0$. From triangle inequality, we have
        \[
            \abs{\abs{a_n} - \abs{L}} \leq \abs{a_n - L} < \epsilon \implies -\epsilon < \abs{a_n} - \abs{L} \quad \forall n \geq n_0
        \]
        Substituting the value of $\epsilon$, we get that
        \[
            \abs{a_n} > \frac{\abs{L}}{2}
        \]
        for all $n \geq n_0$, as desired.
    \end{soln}
    
    \item[9] For given sequences $\{a_n\}_{n \geq 1}$ and $\{b_n\}_{n \geq 1}$, prove or disprove the following statements:
    \begin{enumerate}[label = (\roman*)]
        \item $\{a_n b_n\}_{n \geq 1}$ is convergent if $\{a_n\}_{n \geq 1}$ is convergent.
        \item $\{a_n b_n \}_{n \geq 1}$ is convergent if $\{a_n\}_{n \geq 1}$ is convergent and $\{b_n\}_{n \geq 1}$ is bounded.
    \end{enumerate}
    
    \begin{soln}
        This is a relatively short question. Both the statements are \textbf{false}. Verify that $a_n \coloneqq 1$ and $b_n \coloneqq (-1)^n$ acts as a counterexample for both the statements.
    \end{soln}
    
    \newpage
    
    \item[11] Let $f,g \colon (a,b) \rightarrow \mathbb{R}$ be functions and suppose that $\lim\limits_{x \to c} f(x) = 0$ for $c \in [a,b]$. Prove or disprove the following statements. 
    \begin{enumerate}[label = (\roman*)]
        \item $\lim\limits_{x \to c} \left[ f(x) g(x) \right] = 0$.
        \item $\lim\limits_{x \to c} \left[ f(x) g(x) \right] = 0$ if $g$ is bounded.
        \item $\lim\limits_{x \to c} \left[ f(x) g(x) \right] = 0$ if $\lim\limits_{x \to c} g(x)$ exists.
    \end{enumerate}
    \begin{soln}
    
        \begin{enumerate}[label = (\roman*), labelwidth=!, labelindent=0pt]
            \item This statement is \textbf{false}. As a counterexample, define $a=-1, b=1$ and $c=0$. Define $f,g \colon (-1,1) \rightarrow \mathbb{R}$ as
            \[
                f(x) = x \quad \text{ and } \quad g(x) = \begin{cases}
                    1 & \text{ if } x=0 \\
                    \frac{1}{x^2} & \text{ if } x \neq 0
                \end{cases}
            \]
            Clearly, $\lim\limits_{x \to 0} f(x) = 0$. However, $\lim\limits_{x \to 0} \left[ f(x) g(x) \right]$ does not exist.
            
            \item This statement is \textbf{true}. Since $g$ is bounded, there exists $M > 0$ such that
            \[
                \abs{g(x)} \leq M 
            \]
            for all $x \in (a,b)$. Thus, we have 
            \[
                0 \leq \abs{f(x) g(x)} \leq M \abs{f(x)}
            \]
            for all $x \in (a,b)$. Using Sandwich Theorem, we see that
            \[
                \lim_{x \to c} \; \abs{f(x) g(x)} = 0
            \]
            which in turn implies that
            \[
                \lim_{x \to c} \; [f(x) g(x)] = 0
            \]
            
            \item This statement is \textbf{true}. Since $\lim\limits_{x \to c} g(x)$ exists, we have $\lim\limits_{x \to c} \left[ f(x) g(x) \right] = \lim\limits_{x \to c} f(x) \cdot \lim\limits_{x \to c} g(x) = 0$.
        \end{enumerate}
    \end{soln}
    
    %\item[13 (iii)] Discuss the continuity of the following function :
    %\[
        %f(x) = \begin{cases}
            %\frac{x}{\lfloor x \rfloor} & \text{ if } 1 \leq x < 2 \\
            %1 & \text{ if } x = 2 \\
            %\sqrt{6-x} & \text{ if } 2 < x \leq 3
        %\end{cases}
    %\]
    
    %\begin{soln}
        %We only need to check continuity of $f$ at $x=2$. It is trivially at all other points in the interval $[1,3]$. Evaluating the left hand side limit, we see that $\lim\limits_{x \to 2^-} f(x) = \lim\limits_{x \to 2^-} \ddfrac{x}{1} = 2$. Evaluating the right hand side limit, we see that $\lim\limits_{x \to 2^+} f(x) = \lim\limits_{x \to 2^+} \sqrt{6-x} = 2$. Thus, the limits on both sides exist and are equal to $2$. Hence, $\lim\limits_{x \to 2}f(x) = 2$. Note however that $f(2) = 1 \neq 2 =  \lim\limits_{x \to 2}f(x)$. Thus, $f$ is \textbf{not} continuous at $x=2$ (it has a removable discontinuity). 
    %\end{soln}
\end{enumerate}

\newpage\section{Week 2}
\begin{center}
	2nd December, 2020
\end{center}
\textbf{Sheet 1.}
\begin{enumerate}[leftmargin=*]
    \itemsep0.5em
    \item[13 (ii)] Discuss the continuity of the following function :
    \[
        f(x) = \begin{cases}
            x \sin\left( \ddfrac{1}{x} \right) & \text{ if } x \neq 0 \\
            0 & \text{ if } x=0
        \end{cases}
    \]
    \begin{soln}
        At all points other than $x=0$, the given function is trivially continuous (since it is the product and composition of continuous functions). All that remains is to check the continuity of $f$ at the point $x=0$. Note that 
        \[
            \abs{f(x)} = \abs{x \sin\left( \frac{1}{x} \right)} \leq \abs{x}
        \]
        for all $x \neq 0$. Thus, we have
        \[
            0 \leq \abs{f(x)} \leq \abs{x}
        \]
        Utilising Sandwich Theorem, we see that
        \[
            \lim_{x \to 0} \; f(x) = 0
        \]
        Since $f(0)$ is given to be $0$, we see that $\lim\limits_{x \to 0} f(x) = f(0)$, proving continuity of $f$ at $x=0$. Thus, $f$ is continuous everywhere.
    \end{soln}
    
    \newpage 
    
    \item[15] Let $f \colon \mathbb{R} \rightarrow \mathbb{R}$ be defined as follows. 
    \[
        f(x) = \begin{cases}
            x^2 \sin\left( \ddfrac{1}{x} \right) & \text{ if } x \neq 0 \\
            0 & \text{ if } x=0
        \end{cases}
    \]
    Show that $f$ is differentiable on $\mathbb{R}$. Is $f^{\prime}$ a continuous function?
    \begin{soln}
        Clearly, $f$ is differentiable for all $x \neq 0$. Using the chain rule and product rule, we compute $f^{\prime}$ as 
        \[
            f^{\prime}(x) = 2x \sin\left( \frac{1}{x} \right) - \cos\left( \frac{1}{x} \right)
        \]
        for $x \neq 0$. Now, all that remains to be checked is the differentiability of $f$ at $x=0$. We have
        \[
            \lim_{h \to 0} \; \frac{f(h) - f(0)}{h} = \lim_{h \to 0} \; h \sin{\left( \frac{1}{h}\right)}
        \]
        From the previous question, this limit exists and is equal to $0$. Thus, $f$ is differentiable on all of $\mathbb{R}$ and its derivative is defined as
        \[
            f^{\prime}(x) = \begin{cases}
                2x \sin \left( \ddfrac{1}{x} \right) - \cos \left( \ddfrac{1}{x} \right) & \text{ if } x \neq 0 \\
                0 & \text{ if } x = 0
            \end{cases}
        \]
        Clearly, $f^{\prime}$ is continuous at all $x \neq 0$. All that remains is to check continuity of $f^{\prime}$ at $x=0$. It turns out that $f^{\prime}$ is in fact \textit{not} continuous at $x=0$. We will use the sequential criterion of continuity to prove this. Consider the sequence:
        \[
            x_n \coloneqq \frac{1}{2n\pi}, \quad n \in \mathbb{N}
        \] Clearly, $x_n \to 0$ as $n \to \infty$. However, 
        \[
            f^{\prime}(x_n) = \cancel{\frac{2}{2n\pi} \cdot \sin\left( 2n\pi \right)} - \cos \left( 2n \pi \right) = -1
        \]
        We see that $\lim\limits_{n \to \infty} f(x_n)$ is $-1$, which is not equal to $f^{\prime}(0)$. Hence, $f^{\prime}$ is not continuous at $x=0$. This is an example of a differentiable function whose derivative is not continuous.
    \end{soln}
    
    \newpage 
    
    \item[18] Let $f \colon \mathbb{R} \rightarrow \mathbb{R}$ satisfy
    \[
        f(x+y) = f(x) \cdot f(y)  \; \text{ for all } x,y \in \mathbb{R}
    \]
    If $f$ is differentiable at $0$, then show that $f$ is differentiable at every $c \in \mathbb{R}$ and $f^{\prime}(c) = f^{\prime}(0) \cdot f(c)$.
    \begin{soln}
        We have that $f(x+y) = f(x) \cdot f(y)$ for all $x,y \in \mathbb{R}$. On substituting $x=y=0$, we obtain 
        \[
            f(0) = f(0) \cdot f(0) \implies f(0) = 0 \text{ or } 1
        \]
        First, we consider the case that $f(0)=0$. We have
        \[
            f(x) = f(x+0) = f(x) \cdot f(0) \implies f(x) = 0
        \]
        for all $x$. Thus, $f \equiv 0$ is trivially differentiable and $f^{\prime}(c) = 0 = f^{\prime}(0) \cdot f(c)$ for all $c \in \mathbb{R}$. 
        \medskip
        
        Now consider that $f(0) = 1$. For all $c \in \mathbb{R}$, we have
        \[
            \lim_{h \to 0} \; \frac{f(c+h) - f(c)}{h} = \lim_{h \to 0} \; \frac{f(c)f(h) - f(c) f(0)}{h} = f(c) \cdot \left( \lim_{h \to 0} \; \frac{f(h) - f(0)}{h} \right)
        \]
        If $f$ is differentiable at $0$, then the above limit exists. Thus, if $f$ is differentiable at $0$, then it is differentiable at every $c \in \mathbb{R}$ and $f^{\prime}(c) = f^{\prime}(0) \cdot f(c)$.
    \end{soln}
    
    \newpage
    
    \textbf{Optional Exercises.}
    
    \item[7] Let $f \colon (a,b) \rightarrow \mathbb{R}$ and $c \in (a,b)$. Show that the following statements are equivalent. 
    \begin{enumerate}[label = (\roman*)]
        \item $f$ is differentiable at $c$.
        \item There exists $\delta>0$, \textcolor{blue}{$\alpha \in \mathbb{R}$} and a function $\epsilon_1 \colon (-\delta, \delta) \rightarrow \mathbb{R}$ such that $\lim\limits_{h \to 0} \epsilon_1(h) = 0$ and
        \[
            f(c+h) = f(c) + \alpha h + h \epsilon_1(h)
        \]
        for all $h \in (-\delta, \delta)$.
        \item There exists $\alpha \in \mathbb{R}$ such that
        \[
            \lim_{h \to 0} \left( \frac{\abs{f(c+h) - f(c) - \alpha h}}{\abs{h}} \right) = 0
        \]
    \end{enumerate}
    \begin{soln}
        To show the equivalence of statements $(i)$-$(iii)$, we must show that every statement implies every other statement, that is, a total of $6$ implications. However, we can get away with just showing three implications. We will show that $(i) \Rightarrow (ii)$, $(ii) \Rightarrow (iii)$ and $(iii) \Rightarrow (i)$. This is sufficient to conclude the equivalence of the three statements. (Why?)
        \medskip
        
        $(i) \Rightarrow (ii)$ : Since we are given that $f$ is differentiable at $c$, $f^{\prime}(c)$ exists. We first pick $\delta \coloneqq \min\left\{c-a, b-c\right\}$. Clearly $\delta>0$ and $(c-\delta, c+\delta) \subset (a,b)$. Now, since $f$ is differentiable at $c$, $f^{\prime}(c)$ exists. Define $\alpha \coloneqq f^{\prime}(c)$ and 
        \[
            \epsilon_1(h) = \begin{cases}
                \ddfrac{f(c+h) - f(c) - \alpha h}{h} & \text{ if } h \neq 0 \\
                0 & \text{ if } h = 0
            \end{cases}
        \]
        Since $(c-\delta, c+\delta) \subset (a,b)$, $f(c+h)$ is well defined for all $h \in (-\delta, \delta)$. Now, 
        \[
            \lim_{h \to 0} \; \epsilon_1(h) = \underbrace{\left( \lim_{h \to 0} \; \frac{f(c+h) - f(c)}{h} \right)}_{\alpha} - \alpha = 0
        \]
        Further, some simple algebraic manipulation yields that $f(c+h) = f(c) + \alpha h + h\epsilon_1(h)$ for $h \in (-\delta, \delta), h \neq 0$. Verify that this equation also holds for $h=0$. It then follows that $f(c+h) = f(c) + \alpha h + h\epsilon_1(h)$ for all $h \in (-\delta, \delta)$ and $\lim\limits_{h \to 0} \epsilon_1(h) = 0$, as desired.
        
        \medskip
        
        $(ii) \Rightarrow (iii)$ : By $(ii)$, we have the existence of $\delta>0, \alpha \in \mathbb{R}$ and the function $\epsilon_1$. We have
        \[
            \lim_{h \to 0} \frac{\abs{f(c+h) - f(c) - \alpha h}}{\abs{h}} = \lim_{h \to 0} \abs{\epsilon_1(h)} = 0
        \]
        
        \medskip
        
        $(iii) \Rightarrow (i)$ : By $(iii)$, we have the existence of some $\alpha \in \mathbb{R}$ such that
        \[
            \lim_{h \to 0} \; \frac{\abs{f(c+h) - f(c) - \alpha h}}{\abs{h}} = 0
        \]
        Now, 
        \[
            \lim_{h \to 0} \; \abs{\frac{f(c+h) - f(c)}{h} - \alpha} = 0 \implies \lim_{h \to 0} \; \frac{f(c+h) - f(c)}{h} = \alpha
        \]
        Thus, $f$ is differentiable at $c$, as desired.
        \medskip
        
        Since we have shown $(i) \Rightarrow (ii)$, $(ii) \Rightarrow (iii)$ and $(iii) \Rightarrow (i)$, we get that the three statements are thus equivalent.
    \end{soln}
    
    \item[10] Show that any continuous function $f \colon [0,1] \rightarrow [0,1]$ has a fixed point. \textcolor{blue}{$x$ is said to be a fixed point of $f$ if $f(x) = x$}
    \begin{soln}
        Consider the function $g(x) = f(x) - x$. A fixed point of $f$ is then a root of $g$. Note that $g$ is continuous. Since $0 \leq f(x) \leq 1$ for all $x \in [0,1]$, we have
        \[
            g(0) = f(0) \implies g(0) \geq 0
        \]
        and
        \[
            g(1) = f(1) - 1 \implies g(1) \leq 0
        \]
        First consider the case that at least one of the two equalities hold. That is, either $g(0) = 0$ or $g(1) = 0$ or both. In either of the three cases, we have at least one fixed point ($0$ or $1$ or both, respectively). Now, consider that $g(0) > 0$ and $g(1) < 0$. Since $g$ is continuous, we can appeal to Intermediate Value Theorem. By IVT, there exists some $x_0 \in (0,1)$ such that $g(x_0) = 0$. This point $x_0$ is also a fixed point of $f$. Thus, we have shown that any continuous function mapping the unit interval to itself has a fixed point, as desired.
    \end{soln}
    
    \newpage
    
    \textbf{Sheet 2.}
    
    \item[3] Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$. If $f(a)$ and $f(b)$ are of different signs and $f^{\prime}(x) \neq 0$ for all $x \in (a,b)$, then show that there is a unique $x_0 \in (a,b)$ such that $f(x_0) = 0$. 
    
    \begin{soln}
        Since $f(a)$ and $f(b)$ are of opposite signs and $f$ is continuous, we know that there exists \textbf{at least} one $x_0 \in (a,b)$ such that $f(x_0) = 0$ (by IVP). Now, assume that there was some $y_0 (\neq x_0)$ in $(a,b)$ such that $f(y_0) = 0$. We now have $f(x_0) = f(y_0)$. By Rolle's Theorem, there must exist some $c \in (x_0, y_0)$ such that $f^{\prime}(c) = 0$. Since this $c$ also lies in $(a,b)$, we arrive at a contradiction. Hence, there is a unique $x_0$ in $(a,b)$ such that $f(x_0) = 0$, as desired.
    \end{soln}
    
    \item[5] Use the MVT to show that $\abs{\sin(a) - \sin(b)} \leq \abs{a-b}$ for all $a,b \in \mathbb{R}$. 
    \begin{soln}
        We will break this problem into two cases. First, consider $a = b$. The inequality is trivially satisfied in this case. 
        Next, consider $a \neq b$. Define $f(x) = \sin(x)$. By MVT, there exists some $c$ between $a$ and $b$ such that
        \[
            f^{\prime}(c) = \frac{f(a) - f(b)}{a-b}
        \]
        Since $f^{\prime} = \cos$, we take modulus on both sides to obtain
        \[
            \abs{\frac{\sin a - \sin b}{a - b}} = \abs{\cos c} \leq 1
        \]  
        Rearranging, we get
        \[
            \abs{\sin a - \sin b} \leq \abs{a - b}
        \]
        for all $a,b \in \mathbb{R}$, as desired.
    \end{soln}
\end{enumerate}

\newpage\section{Week 3}
\begin{center}
	9th December, 2020
\end{center}
\textbf{Sheet 2.}

\begin{enumerate}[leftmargin=*]
    \itemsep0.5em
    \item[8] In each case, find a function $f$ that satisfies all the given conditions, or else show that no such function exists.
    \begin{enumerate}
        \item[(ii)] $f^{\prime \prime}(x) \geq 0$ for all $x \in \mathbb{R}$, $f^{\prime}(0) = 1$, $f^{\prime}(1) = 2$.
        \item[(iii)] $f^{\prime\prime}(x) \geq 0$ for all $x \in \mathbb{R}$, $f^{\prime}(0) = 1$, $f(x) \leq 100$ for all $x>0$.
    \end{enumerate}
    
    \medskip
    
    \begin{soln}$ $\par\nobreak\ignorespaces
        \begin{enumerate}[leftmargin=*]
            \item[(ii)] Possible. Verify that $f \colon \mathbb{R} \rightarrow \mathbb{R}$ with $f(x) \coloneqq x + \ddfrac{x^2}{2}$ is one such function.
            
            \item[(iii)] Not possible. Assume that it was indeed possible to find such a function $f$. Then, we are given that $f^{\prime \prime}$ exists everywhere. Thus, $f^{\prime}$ is continuous and differentiable everywhere. As $f^{\prime \prime}$ is non-negative, $f^{\prime}$ must be increasing everywhere. Since $f^{\prime}(0) = 1$, we have that $f^{\prime}(c) \geq 1$ for all $c>0$.
            
            Let $x \in (0,\infty)$. By MVT, there exists $c \in (0,x)$ such that \[
                f^{\prime}(c) = \frac{f(x) - f(0)}{x - 0}
            \]
            Since $c>0$, we have $f^{\prime}(c) \geq 1$ as shown above. Thus, $f(x) - f(0) \geq x$ for all $x > 0$. However, consider $x_0 \coloneqq \max\left(101-f(0), 1\right)$. Clearly, $x_0 > 0$ (as it is $\geq 1$). Also, $f(x_0) > 100$, which contradicts the condition that $f(x) \leq 100$ for all $x>0$. Hence, no such $f$ can exist.
        \end{enumerate}
    \end{soln}
    
    \newpage
    
    \item[10 (i)] Sketch the following curves after locating intervals of increase/decrease, intervals of concavity upward/downward, points of local minima/maxima, points of inflection and \textcolor{blue}{asymptotes}. How many times and approximately where does the curve cross the x-axis?
    \[
        y = 2x^3 + 2x^2 - 2x - 1
    \]
    
    \begin{soln}
        We are given 
        \[
            f(x) = 2x^3 + 2x^2 - 2x - 1
        \]  
        On differentiating, we get
        \[
            f^{\prime}(x) = 6x^2 + 4x - 2 = 2(x+1)(3x-1)
        \]
        Thus, $f^{\prime} > 0$ in $(-\infty, -1) \cup (\frac{1}{3}, \infty)$ and $f$ is strictly increasing here. $f^{\prime} < 0$ in $(-1, \frac{1}{3})$ and $f$ is strictly decreasing here. Thus, $f$ has a local maximum at $-1$ and a local minimum at $\frac{1}{3}$. Differentiating again, we see that
        \[
            f^{\prime\prime}(x) = 12x + 4
        \]
        Thus, $f$ is convex in $(-\frac{1}{3}, \infty)$ and concave in $(-\infty, -\frac{1}{3})$, with a point of inflection at $-\frac{1}{3}$. A curve for $f$ can be sketched as follows
        
        \begin{figure}[!h]
    		\centering
    		\begin{tikzpicture}[scale=3.2]
    			\draw[step=0.5cm,gray,very thin] (-1.6,-1.6) grid (1.1,1.1);
    			\draw[thick,->] (0,0) -- (1,0);
    			\draw[thick,->] (0,0) -- (0, 1);
    			\draw[thick,->] (0,0) -- (-1.5,0);
    			\draw[thick,->] (0,0) -- (0,-1.5);
    			\draw[thin,-,red] (-1,1) -- (-1,-1.5);
    			\draw[thin,-,red] (0.333,1) -- (0.333,-1.5);
    			\draw[thin,-,blue] (-0.333,1) -- (-0.333,-1.5);
    			\node[] at (1.1, 0) {\tiny $x$};
    			\node[] at (0, 1.1) {\tiny $f(x)$};
    			\foreach \x in {-1.5,-1,-0.5,0,0.5, 1}
       \draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north] {\tiny $\x$};
    \foreach \y in {-1.5,-1,-0.5,0,0.5,1}
        \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {\tiny $\y$};
    
        \draw[variable=\t,domain=-1.4:0.9,samples=500] plot ({\t},{2*\t*\t*\t + 2*\t*\t - 2*\t - 1});
    
    		\end{tikzpicture}
	\end{figure}
    \end{soln}
    
    \newpage
    
    \item[11] Sketch a continuous function having all the following properties :
    \begin{description}
        \itemsep0.05em
        \item $f(-2)=8, f(0)=4, f(2)=0; f^{\prime}(-2) = f^{\prime}(2) = 0;$
        \item $f^{\prime}(x)>0$ for $\abs{x}>2$, $f^{\prime}(x)<0$ for $\abs{x} < 2$;
        \item $f^{\prime\prime}(x) < 0$ for $x<0$, $f^{\prime\prime}(x)>0$ for $x>0$.
    \end{description}
    
    \begin{soln}
        $f^{\prime} > 0$ in $(-\infty, -2) \cup (2, \infty)$ and thus $f$ is strictly increasing here. $f^{\prime} < 0$ in $(-2, 2)$ and thus $f$ is strictly decreasing here. Thus, $f$ has a local maximum at $-2$ and a local minimum at $-2$. The function values at these points are $8$ and $0$ respectively. Also, $f$ is convex in $(0, \infty)$ and concave in $(-\infty, 0)$ with an inflection point at $0$. Putting all these together, we can sketch a curve for $f$ as:
        
        \begin{figure}[!h]
		\centering
		\begin{tikzpicture}[scale = 1.25]
			\draw[step=1cm,gray,very thin] (-4.1,-1.1) grid (4.1,8.1);
			\draw[thick,->] (0,0) -- (4,0);
			\draw[thick,->] (0,0) -- (0, 8);
			\draw[thick,->] (0,0) -- (-4,0);
			\draw[thin,-,red] (-2,-1) -- (-2,8);
			\draw[thin,-,red] (2,-1) -- (2,8);
			\draw[thin,-,blue] (0,-1) -- (0,8);
			\node[] at (4.2, 0) {\tiny $x$};
			\node[] at (0, 8.2) {\tiny $f(x)$};
			\foreach \x in {-3, ..., 3}
   \draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north] {\tiny $\x$};
\foreach \y in {1, ...,7}
    \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {\tiny $\y$};

    \draw[variable=\t,domain=-3.9:3.9,samples=500] plot ({\t},{0.25*\t*\t*\t - 3*\t + 4});

		\end{tikzpicture}
	\end{figure}
    \end{soln}
    
    \newpage
    
    \textbf{Sheet 3.}
    
    \item[1 (ii)] Write down the Taylor expansion of $\arctan(x)$ around the point $0$. Also write a precise remainder term $R_n(x)$.
    
    \begin{soln}
        Let $f$ denote the arctangent function. Let $g$ denote its derivative
        \[
            g(x) = f^{\prime}(x) = \frac{1}{1+x^2}
        \]
        For $\abs{x} < 1$, we can expand the latter as a geometric series. Thus, we have
        \[
            g(x) = 1 - x^2 + x^4 - x^6 + \ldots = \sum_{k=0}^{\infty} (-1)^k x^{2k}
        \]
        for $\abs{x} < 1$. Let us now evaluate the $n^{\text{th}}$ derivative of $f$ at $x=0$. For $n \geq 1$, we have
        \[
            f^{(n)} = g^{(n-1)}
        \]
        where $f^{(r)}$ and $g^{(r)}$ denote the $r^{\text{th}}$ derivatives of $f$ and $g$ respectively. To evaluate the derivatives of $g$, we will consider two cases. First, we will evaluate all odd derivatives (derivatives of the order $2n-1$). On differentiating $g$, $r$ times, we will be left with a power series where the powers of $x$ are of the form $(2k-r)$ \textcolor{blue}{for integer $k$}. When $r$ is odd, no exponent of $x$ vanishes. As a result, all the terms of the power series vanish when we plug in $x=0$. Thus, all odd derivatives of $g$ vanish at $0$. I leave it to you to compute the even order derivatives at $x=0$. The derivatives of $g$ at $0$ are then given by
        \[
            g^{(2n-1)}(0) = 0, \quad g^{(2n)}(0) = (-1)^n \cdot (2n)!
        \]
        for $n \geq 1$. Now, we have
        \[
            f^{2n}(0) = g^{(2n-1)}(0) = 0
        \]
        and
        \[
            f^{(2n-1)}(0) = g^{(2n-2)}(0) = (-1)^{n-1} \cdot (2n-2)!
        \]
        for $n \geq 1$. \textcolor{blue}{We shall first compute the zeroth Taylor Polynomial. We have}
        \[
            \textcolor{blue}{T_0(x) = f(0) = 0}
        \]  
        Let us now compute the $n^{\text{th}}$ Taylor polynomial $T_n(x)$ of $f$ at $0$ \textcolor{blue}{for $n \geq 1$}. Define $M \coloneqq \left\lfloor \left( \frac{n+1}{2} \right)  \right\rfloor$.   \textcolor{blue}{ For $n \geq 1$}, we then have
        \[
            T_n(x) = \sum_{k=0}^{n} \frac{f^{(k)}(0)}{k!} x^k
        \]
        where $f^{(0)} = f$. With a bit of manipulation, we can write 
        \[
            T_n(x) = \sum_{k=1}^{M} \frac{(-1)^{k-1} \cdot (2k-2)!}{(2k-1)!} \cdot x^{2k-1}
        \]  
        Thus, the $n^{\text{th}}$ Taylor polynomial for $\arctan$ at $0$ is given by
        \[
            T_n(x) = \sum_{k=1}^{M} \frac{(-1)^k}{2k-1} x^{2k-1} \quad , \quad M = \left\lfloor \left( \frac{n+1}{2} \right) \right\rfloor
        \]
        
        Writing it out in a neater way, we have
        \[
            T_{2n-1}(x) = x - \frac{x^3}{3} + \ldots + \frac{(-1)^{n-1}}{2n-1} x^{2n-1} 
        \]
        and 
        \[
            T_{2n}(x) = T_{2n-1}(x)
        \]
        
        The remainder term is then just the difference of the arctangent function at $x$ and its Taylor polynomial. More precisely, we have
        \[
            R_n(x) = \arctan(x) - \sum_{k=0}^{M} \frac{(-1)^k}{2k-1} x^{2k-1}
        \]
        with $M$ defined as previously. Let us now calculate the remainder term $R_{2n-1}(x)$ more explicitly. We have
        \[
            \arctan^{\prime} = \textcolor{cyan}{1 - x^2 + x^4 + \ldots + (-1)^{n-1} x^{2n-2}} + (-1)^n x^{2n} \left[1 - x^2 + x^4 - \ldots \right]
        \]
        \[
            \therefore \arctan^{\prime} = \textcolor{cyan}{1 - x^2 + x^4 + (-1)^{n-1} x^{2n-2}} + (-1)^{n} \frac{x^{2n}}{1+x^2}
        \]
        On integrating both sides from $0$ to $x$, the cyan-coloured term just becomes $T_{2n-1}(x)$. (Verify!) Thus, we have
        \[
            \arctan(x) = T_{2n-1}(x) + (-1)^n \int_{0}^{x} \frac{t^{2n}}{1+t^2} \, \D t
        \]
        Thus, 
        \[
            R_{2n-1}(x) = (-1)^n \int_{0}^{x} \frac{t^{2n}}{1+t^2} \, \D t
        \]
        and 
        \[
            R_{2n}(x) = R_{2n-1}(x)
        \]
    \end{soln}
    
    \newpage
    
    \item[2] Write down the Taylor series of the polynomial $x^3 - 3x^2 + 3x - 1$ about the point $1$.
    \begin{soln}
        The Taylor series is just $(x-1)^3$. Let us see why. We wish to expand 
        \[
            f(x) = x^3 - 3x^2 + 3x -1
        \]
        about the point $a=1$. We have
        \[
            f(1) = 0
        \]
        \[
            f^{(1)}(1) = 0
        \]
        \[
            f^{(2)}(1) = 0
        \]
        \[
            f^{(3)}(1) = 6
        \]
        \[
            f^{(n)}(1) = 0 \; \text{ for all } n \geq 4
        \]
        Thus, we have
        \[
            P_0(x) = P_1(x) = P_2(x) = 0
        \]
        \[
            P_3(x) = \frac{6}{3!} (x-1)^3 = (x-1)^3
        \]
        and
        \[
            P_n(x) = P_3(x) \; \text{ for all } n \geq 4
        \]
        We also have
        \[
            R_n(x) \coloneqq f(x) - P_n(x) = 0 \; \text{ for all } n \geq 3
        \]
        Thus, $R_n(x) \to 0$ for \textbf{all} $x$. Thus, the Taylor series of the function about the point $1$ is simply given by $(x-1)^3$.
    \end{soln}
    
    \item[4] Consider the series $\sum\limits_{k=0}^{\infty} \ddfrac{x^k}{k!}$ for a fixed $x$. Prove that it converges as follows. Choose $N > 2\textcolor{blue}{\abs{x}}$. We see that for $n > N$, 
    \[
        \textcolor{blue}{\abs{\frac{x^{n+1}}{(n+1)!}} < \frac{1}{2} \cdot \abs{\frac{x^n}{n!}}}
    \]
    It should now be relatively easy to show that the given series is Cauchy, and hence (by the completeness of $\mathbb{R}$) is convergent.
    
    \begin{soln}
        Let the partial sums of the series be denoted as $S_m(x)$. That is,
        \[
            S_m(x) \coloneqq \sum_{k=0}^{m} \frac{x^k}{k!}
        \]
        We wish to show that the difference $\abs{S_m(x) - S_n(x)}$ can be made arbitrarily small whenever $m$ and $n$ are sufficiently large. Assume that $m>n>N$. We see that
        \[
            \abs{S_m(x) - S_n(x)} = \abs{\sum_{k=n+1}^{m}\frac{x^k}{k!}} \leq \abs{\frac{x^n}{n!}} \left( \frac{1}{2} + \frac{1}{4} + \ldots + \frac{1}{2^{m-n}} \right) \leq \abs{\frac{x^n}{n!}} \textcolor{blue}{< \abs{\frac{x^N}{N!}}}
        \]
        Now for any $\epsilon>0$, we can pick $\textcolor{blue}{N}$ large enough such that
        \[
            \abs{\frac{x^N}{N!}} < \epsilon
        \]
        This is possible because the sequence
        \[
            a_n = \frac{\abs{x}^n}{n!} 
        \]
        is convergent (it is eventually decreasing and bounded below) and its limit is $0$. Thus, for all $m>n>N$, we have
        \[
            \abs{S_m(x) - S_n(x)} < \epsilon
        \]
        Hence, the given series is Cauchy and thus convergent.
        
        (\underline{Remark}: During the tutorial session, I had showed that the term $\abs{S_m(x) - S_n(x)}$ can be made arbitrarily small by picking $\textcolor{blue}{n}$ large enough. However, this is incorrect! We want to show that the term is smaller than $\epsilon$ for any $n,m$ greater than $N$. So really we have to make $\textcolor{blue}{N}$ large enough and conclude. This is what I have now done.)
    \end{soln}
    
    \newpage
    
    \item[5] Using Taylor series, write down a series for the integral
    \[
        \int \frac{e^x}{x} \, \D x
    \]
    
    \begin{soln}
        We will assume that a Taylor series can be integrated term by term and then proceed. Recall that the Taylor series for $e^x$ is given by
        \[
            e^x = \sum_{k=0}^{\infty} \frac{x^k}{k!}
        \]
        We have
        \begin{align*}
            \int \frac{e^x}{x} \, \D x &= \int \left( \frac{1}{x} + \sum_{k=1}^{\infty} \frac{x^{k-1}}{k!} \right) \, \D x \\
            &= \int \frac{1}{x} \, \D x + \int \left( \sum_{k=1}^{\infty} \frac{x^{k-1}}{k!} \right) \, \D x
        \end{align*}
        Since the latter term is a Taylor series, we can integrate it term by term to obtain
        \[
             \int \frac{e^x}{x} \, \D x = \log x + \sum_{k=1}^{\infty} \left( \int \frac{x^{k-1}}{k!} \, \D x \right)
        \]
        Thus, a series representation of the integral is given by
        \[
            \int \frac{e^x}{x} \, \D x = \log x + \sum_{k=1}^{\infty} \frac{x^k}{k \cdot k!}
        \]
    \end{soln}
\end{enumerate}

\newpage\section{Week 4}
\begin{center}
	16th December, 2020
\end{center}
\textbf{Sheet 4.}

\begin{enumerate}[leftmargin=*]
    \itemsep0.5em
    \item[2 (a)] Let $f \colon [a,b] \rightarrow \mathbb{R}$ be Riemann integrable and $f(x) \geq 0$ for all $x \in [a,b]$. Show that $\displaystyle\int_a^b f(x) \, \D x \geq 0$. Further, if $f$ is continuous and $\displaystyle\int_a^b f(x) \, \D x = 0$, show that $f(x) = 0$ for all $x \in [a,b]$.
    
    \begin{soln}
        Let $P = \left\{ a=x_0 <  x_1 < \ldots < x_n = b \right\}$ denote a partition of $[a,b]$. Define $\Delta x_i = x_i - x_{i-1}$ for $1 \leq i \leq n$. Further, we define
        \[
            m_i = \inf \left\{ f(x) \colon x_{i-1} \leq x \leq x_i \right\}
        \]
        Since $f(x) \geq 0$ for all $x \in [a,b]$, it follows that $m_i \geq 0$ for all $i$. The lower sum is now defined as
        \[
            L(P,f) = \sum_{i=1}^{n} m_i \Delta x_i
        \]
        Since $m_i \geq 0$ and $\Delta x_i > 0$ for all $i$, it follows that $L(P, f) \geq 0$ for any partition $P$. Thus, we also see that $L(f) \geq 0$ since $L(f)$ is the supremum of $L(P,f)$ over all partitions $P$. Since $f$ is Riemann integrable, we have
        \[
            \int_a^b f(x) \, \D x = L(f) \geq 0
        \]
        as desired. 
        
        \medskip
        
        Now, let us further assume that $f$ is continuous and that $\int_a^b f(x) \, \D x = 0$. If $f$ is not identically zero, then there exists $c \in [a,b]$ such that $f(c) > 0$. Continuity of $f$ implies that there exists a $\delta > 0$ such that, if $x \in [a,b]$, 
        \[
            \abs{x-c} < \delta \implies \abs{f(x) - f(c)} < \frac{f(c)}{2} \implies f(x) > \frac{f(c)}{2}
        \]
        We may now assume $c \in {\color{red}(}a,b{\color{red})}$ without any loss of generality \footnote{If $c=a$ or $c=b$, then we can pick another point $\Tilde{c}$ in $(a,b)$ such that $f(\Tilde{c}) \neq 0$.} Further, pick $\delta > 0$ small enough so that $(c-\delta, c+\delta) \subset (a,b)$. Now, consider the partition
        \[
            P = \left\{ a, c-\frac{\delta}{2}, c+\frac{\delta}{2}, b \right\}
        \]
        Since we have
        \[
            \inf\limits_{x \in [c-\frac{\delta}{2}, c+\frac{\delta}{2}]} f(x) \geq \frac{f(c)}{2}
        \]
        it follows that
        \[
            L(f) \geq L(P,f) \geq \frac{f(c) \delta}{2} > 0
        \]
        Further, if $f$ is Riemann integrable, we have that its integral over $[a,b]$ is equal to $L(f)$, which is strictly positive - a contradiction! Hence, $f$ must be identically zero.
    \end{soln}
    
            
        \medskip
        \hrule
        
        \medskip
        
    
    \textcolor{blue}{\underline{Alternate.} (easier)} 
    
    \begin{soln}
        Consider the trivial partition $P_0 = {a,b}$ of $[a,b]$. Since $f(x) \geq 0$ for all $x \in [a,b]$, we have
        \[
            \inf_{x \in [a,b]} f(x) \geq 0
        \]
        We have
        \[
            L(f, P_0) = \left[ \inf_{x \in [a,b]} f(x) \right] \cdot (b-a) \geq 0
        \]
        and 
        \[
            L(f) \geq L(f, P_0) \geq 0
        \]
        Since $f$ is Riemann integrable, its integral is $L(f)$, which is non-negative, as desired.
        
        \medskip

        For the second part, define $F \colon [a,b] \to \mathbb{R}$ as
        \[
            F(x) = \int_a^x f(t) \, \D t
        \]
        Since $f$ is continuous, we get that $F$ is differentiable with $F^{\prime} = f$, from the Fundamental Theorem of Calculus (Part 1).
        Since $f \geq 0$, we have $F^{\prime} \geq 0$ and hence, $F$ is increasing. This implies that for all $x \in [a,b]$, we have
        \[
            F(a) \leq F(x) \leq F(b)
        \]
        However, since $F(a) = 0 = F(b)$, we get that $F$ is constant and hence, 
        \[
            f(x) = F^{\prime}(x) = 0
        \]
        for all $x \in [a,b]$, as desired.
    \end{soln}
    
    \newpage
    
    \item[2 (b)] Give an example of a Riemann integrable function on $[a,b]$ such that $f(x) \geq 0$ for all $x \in [a,b]$ and $\int_a^b f(x) \, \D x = 0$, but $f(x) \neq 0$ for some $x \in [a,b]$.
    
    \begin{soln}
        As we saw in the previous question, no continuous function can satisfy these conditions. Thus, we must look for a discontinuous function. We define $f$ on $[0,1]$ as follows:
        \[
            f(x) = \begin{cases}
                0 & \text{when $x \neq \frac{1}{2}$} \\
                1 & \text{when $x = \frac{1}{2}$}
            \end{cases}
        \]
        Since $f$ has only finitely many discontinuities, it is Riemann integrable. Also, $f(x) \geq 0$ for all $x \in [0,1]$. Further, it is easy to show that its Riemann integral over the interval is $0$. Lastly, we have $f(\frac{1}{2}) = 1 \neq 0$. Thus, $f(x) \neq 0$ for some $x \in [0,1]$. Hence, this $f$ satisfies our desired conditions.
    \end{soln}
    
    \newpage


    \item[3] Evaluate $\lim\limits_{n \to \infty} S_n$ by showing that $S_n$ is an \textcolor{red}{\sout{approximate}} appropriate Riemann sum of a suitable function over a suitable interval.
    
        \begin{multicols}{2}
    \begin{enumerate}[leftmargin=*]
        \item[(ii)] $S_n = \sum\limits_{i=1}^{n} \ddfrac{n}{i^2 + n^2}$
        \item[(iv)] $S_n = \dfrac{1}{n} \sum\limits_{i=1}^n \cos \dfrac{i\pi}{n}$
    \end{enumerate}
    \end{multicols}
    
    We shall use the following theorem for both the parts. 
    
    \begin{thm*}
        Let $f \colon [a,b] \to \mathbb{R}$ be Riemann integrable. Suppose that $(P_n, T_n)$ be a sequence of tagged partitions of $[a,b]$ such that $\norm{P_n} \to 0$. Then, 
        \[
            R(P_n, T_n, f) \to \int_a^b f(t) \, \D t
        \]
    \end{thm*}
    
    \begin{enumerate}[leftmargin=*]
    \item[(ii)]
    \begin{soln}
        Consider $f \colon [0,1] \rightarrow \mathbb{R}$ defined as $f(x) \coloneqq \arctan(x)$. Then, we have
        \[
            f^{\prime}(x) = \frac{1}{1+x^2}
        \]
        Since $f^{\prime}$ is continuous on $[0,1]$, it is Riemann integrable on $[0,1]$. Let $P_n \coloneqq \left\{ x_i = \frac{i}{n} \colon 0 \leq i \leq n \right\}$ be a tagged partition of $[0,1]$ for $n \in \mathbb{N}$ and let $T_n \coloneqq \left\{ t_i = \frac{i}{n} \colon 1 \leq i \leq n\right\}$ denote the tags of the partition. 
        
        \medskip
        
        We have $\Delta x_i = x_{i} - x_{i-1} = \frac{1}{n}$ for all $1 \leq i \leq n$. The Riemann sum corresponding to this tagged partition is given by 
        \begin{align*}
            R(P_n, T_n, f^{\prime}) = \sum_{i=1}^{n} f^{\prime}(t_i) \Delta x_i &= \sum_{i=1}^n \frac{1}{1+t_i^2} \cdot \frac{1}{n} \\
            &= \sum_{i=1}^n \frac{1}{1 + \left(\frac{i}{n} \right)^2 } \cdot \frac{1}{n} \\
            &= \sum_{i=1}^n \frac{n}{i^2 + n^2} = S_n
        \end{align*}
        
        Thus, $R(P_n, T_n, f^{\prime}) = S_n$ for all $n \geq 1$. Moreover, 
        \[
            \norm{P_n} = \max \left\{ x_i - x_{i-1} \colon 1 \leq i \leq n \right\} = \frac{1}{n}
        \]
        Clearly, we have
        \[
            \lim_{n \to \infty} \norm{P_n} = 0
        \]
        and thus,
        \[
            \lim_{n \to \infty} S_n = \int_0^1 f^{\prime}(x) \, \D x
        \]
        From the Fundamental Theorem of Calculus (Part $2$), we have that
        \[
            \lim_{n \to \infty} S_n = \int_0^1 f^{\prime}(x) \, \D x = f(1) - f(0) = \boxed{\frac{\pi}{4}}
        \]
        \end{soln}
        
        \item[(iv)] 
        \begin{soln}
    
        Consider $f : [0,1] \rightarrow \mathbb{R}$ defined as 
        \[
            f(x) \coloneqq \frac{1}{\pi} \sin(\pi x) 
        \]
        We then have $f^{\prime}(x) = \cos(\pi x)$. Since $f^{\prime}$ is continuous on $[0,1]$, it is Riemann integrable on $[0,1]$. Let $P_n \coloneqq \left\{ x_i = \frac{i}{n} \colon 0 \leq i \leq n \right\}$ be a tagged partition of $[0,1]$ for $n \in \mathbb{N}$ and let $T_n \coloneqq \left\{ t_i = \frac{i}{n} \colon 1 \leq i \leq n\right\}$ denote the tags of the partition. 
        
        \medskip
        
        We have $\Delta x_i = x_{i} - x_{i-1} = \frac{1}{n}$ for all $1 \leq i \leq n$. The Riemann sum corresponding to this tagged partition is given by 
        \begin{align*}
            R(P_n, T_n, f^{\prime}) = \sum_{i=1}^{n} f^{\prime}(t_i) \Delta x_i &= \sum_{i=1}^n \cos(\pi t_i) \cdot \frac{1}{n} \\
            &= \frac{1}{n} \sum\limits_{i=1}^n \cos \frac{i\pi}{n} = S_n
        \end{align*}
        
        Thus, $R(P_n, T_n, f^{\prime}) = S_n$ for all $n \geq 1$. Moreover, 
        \[
            \norm{P_n} = \max \left\{ x_i - x_{i-1} \colon 1 \leq i \leq n \right\} = \frac{1}{n}
        \]
        Clearly, we have
        \[
            \lim_{n \to \infty} \norm{P_n} = 0
        \]
        and thus,
        \[
            \lim_{n \to \infty} S_n = \int_0^1 f^{\prime}(x) \, \D x
        \]
        From the Fundamental Theorem of Calculus (Part $2$), we have that
        \[
            \lim_{n \to \infty} S_n = \int_0^1 f^{\prime}(x) \, \D x = f(1) - f(0) = \boxed{0}
        \]
        \end{soln}
    \end{enumerate}
    
    \newpage
    
    \item[4(b)] Compute $\dfrac{\D F}{\D x}$ if for $x \in \mathbb{R}$, 
    \begin{multicols}{2}
    \begin{enumerate}[leftmargin=*]
        \item[(i)] $F(x) = \displaystyle\int_1^{2x} \cos\left( t^2 \right) \, \D t$
        \item[(ii)] $F(x) = \displaystyle\int_0^{x^2} \cos(t) \, \D t$
    \end{enumerate}
    \end{multicols}
    
    \begin{soln}
        Before solving these two subparts, I will first prove a short lemma. 
        \begin{lem*}
            Let $f \colon \mathbb{R} \to \mathbb{R}$ be continuous and let $v \colon \mathbb{R} \to \mathbb{R}$ be differentiable. Let $F \colon \mathbb{R} \to \mathbb{R}$ be defined as
            \[
                F(x) \coloneqq \int_{0}^{v(x)} f(t) \, \D t
            \]
            Then, 
            \[
                F^{\prime}(x) = f\left( v(x) \right) \cdot v^{\prime}(x)
            \]
        \end{lem*}
        
        \begin{proof}
            First, we define $G \colon \mathbb{R} \to \mathbb{R}$ as 
            \[
                G(x) \coloneqq \int_0^x f(t) \, \D t
            \]
            Then, $G^{\prime} = f$ by the Fundamental Theorem of Calculus (Part $1$). Now,
            \[
                F(x) = G\left( v(x) \right)
            \]
            A simple application of chain rule yields
            \[
                F^{\prime}(x) = f\left( v(x) \right) \cdot v^{\prime}(x)
            \]
            as desired.
        \end{proof}
        
        \begin{enumerate}[leftmargin=*]
            \item[(i)] We have $v(x) = 2x$ and $f(t) = \cos(t^2)$. It thus follows from the above lemma that
            \[
                \frac{\D F}{\D x} = \cos \left( (2x)^2 \right) \cdot \left( 2x \right)^{\prime} = \boxed{2\cos\left( 4x^2 \right)}
            \]
            \item[(ii)] We have $v(x) = x^2$ and $f(t) = \cos(t)$. It thus follows from the above lemma that
            \[
                \frac{\D F}{\D x} = \cos \left( x^2 \right) \cdot \left( x^2 \right)^{\prime}= \boxed{2x \cos\left( x^2 \right)}
            \]
        \end{enumerate}
    \end{soln}
    
    \newpage
    
    \item[6] Let $f \colon \mathbb{R} \rightarrow \mathbb{R}$ be continuous and $\lambda \in \mathbb{R}, \lambda \neq 0$. For $x \in \mathbb{R}$, let
    \[
        g(x) = \frac{1}{\lambda} \int_0^x f(t) \sin{\lambda(x-t)} \, \D t
    \]
    Show that $g^{\prime\prime}(x) + \lambda^2 g(x) = f(x)$ for all $x \in \mathbb{R}$ and $g(0) = g^{\prime}(0) = 0$.
    
    \begin{soln}
        We will first make use of the identity $\sin{(A-B)} = \sin{A}\cos{B} - \cos{A}\sin{B}$. We have
        \begin{align*}
            g(x) &= \frac{1}{\lambda} \int_0^x f(t) \sin{\lambda(x-t)} \, \D t \\
            &= \frac{1}{\lambda} \int_0^x f(t) \left( \sin{\lambda x} \cos{\lambda t} - \cos{\lambda x} \sin{\lambda t} \right) \, \D t \\
            &= \frac{1}{\lambda} \sin{\lambda x} \int_0^x f(t) \cos{\lambda t} \, \D t - \frac{1}{\lambda} \cos{\lambda t} \int_0^x f(t) \sin{\lambda t} \, \D t
        \end{align*}
        
        On applying the product rule and Fundamental Theorem of Calculus (Part $1$), we get
        \begin{align*}
            g^{\prime}(x) &= \cos{\lambda x} \int_0^x f(t) \cos{\lambda t} \, \D t + \cancel{\frac{1}{\lambda} \sin{\lambda x} \cdot f(x) \cdot \cos{\lambda x}} \\
            &+ \sin{\lambda x} \int_0^x f(t) \sin{\lambda t} \, \D t - \cancel{ \frac{1}{\lambda} \sin{\lambda x} \cdot f(x) \cdot \cos{\lambda x}}
        \end{align*}
        \[
            \therefore \; g^{\prime}(x) = \cos{\lambda x} \int_0^x f(t) \cos{\lambda t} \, \D t + \sin{\lambda x} \int_0^x f(t) \sin{\lambda t} \, \D t
        \]
        It is now easy to verify that both $g(0)$ and $g^{\prime}(0)$ are indeed $0$. We will differentiate $g^{\prime}$ in a similar manner to obtain
        \begin{align*}
            g^{\prime \prime}(x) &= -\lambda \sin{\lambda x} \int_0^x f(t) \cos{\lambda t} \, \D t + f(x) \cos^2{\lambda x} \\
            &+ \lambda \cos{\lambda x} \int_0^x f(t) \sin{\lambda t} \, \D t + f(x) \sin^2{\lambda x} \\
            &= f(x) - \lambda^2 \left( \frac{1}{\lambda} \int_0^x f(t) \left( \sin{\lambda x} \cos{\lambda t} - \cos{\lambda x} \sin{\lambda t} \right) \, \D t\right) \\
            &= f(x) - \lambda^2 g(x)
        \end{align*}
        It thus follows that $g^{\prime\prime}(x) + \lambda^2 g(x) = f(x)$ for all $x \in \mathbb{R}$, as desired.
    \end{soln}
\end{enumerate}
    

\end{document}
